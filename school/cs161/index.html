<HTML><HEAD><META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1"><META NAME="Generator" CONTENT="Microsoft Word 97/98"><TITLE>A Heap Based Priority Queue</TITLE></HEAD><BODY LINK="#0000ff"><FONT FACE="Courier"><P ALIGN="CENTER">A Heap Based Priority Queue</P><P ALIGN="CENTER">By Sriranga Veeraraghavan</P><P ALIGN="CENTER">CS 161 Stanford University Summer 1998</P><P ALIGN="CENTER">9 August 1998</P></FONT><FONT FACE="Courier"><B>1.0 Introduction</B></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P>For this project, I have implemented the specified scheduler framework based on the Heap data structure as described by <U>Cormen, Leierson and Rivest</U>[1] and <U>Segdewick</U>[2].  The Heap was chosen for its simplicity, easy of maintenance and reasonable efficiency.</P><P>I have worked alone on this project.</P>
<FONT FACE="Courier"><B>1.1 Sources</B></FONT><FONT FACE="Courier"><P>The source files for this project are:</P>
<UL>
<LI><A HREF="pqops.c">pqops.c</A> (Priority Queue Operations)
<LI><A HREF="pqops.h">pqops.h</A> 
<LI><A HREF="projlib.h">projlib.h</A> (Project Library Header File)
<LI><A HREF="scheduler.c">scheduler.c</A> (Scheduler)
<LI><A HREF="scheduler.h">scheduler.h</A>
</UL>
<B><P>2.0&#9;Description</P></B></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P>The data structure used in this scheduler implementation is known as a Binary Heap.  This data structure was first described by <U>Williams</U>[3], who described an implementation of a priority queue based on this structure.</P><P>My implementation is partially based on descriptions of the heap operations given by <U>Cormen, Leierson and Rivest</U> [1] and <U>Segdewick</U>[2],  but the following operations were independently implemented:</P><UL><LI>Delete</LI><LI>Increase Key</LI></UL></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P>As discussed latter, my implementations fulfill the O(lg(n)) worst case running time requirement specified by <U>Cormen, Leierson and Rivest</U>[1].  </P><P>In this implementation the heap is represented as a structure which holds three key pieces of information:</P><UL><LI>The current heap size</LI><LI>The max heap size</LI><LI>The nodes in the heap</LI></UL></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P>As specified, the max heap size is determined at run time based on user input.  The nodes in the heap are treated as if they were in an array of the max size and are accessed based on an index.  Each node holds essential information about the process it represents which include:</P><UL><LI>process id</LI><LI>priority</LI><LI>duration</LI><LI>niceness</LI><LI>cpu usage</LI></UL></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P>The nodes are stored in the heap with their priority as the key.  Thus the node (or job) with the highest priority will always be the top most node.</P><B><P>2.1 Priority Queue Operations</P></B></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P>The following priority queue operations have been implemented:</P><UL><LI>Insert (</FONT><FONT FACE="Courier" SIZE=2>heap_insert()</FONT><FONT FACE="Courier">)</LI><LI>Delete (</FONT><FONT FACE="Courier" SIZE=2>heap_delete()</FONT><FONT FACE="Courier">)</LI><LI>Increase Key (</FONT><FONT FACE="Courier" SIZE=2>heap_increase_key()</FONT><FONT FACE="Courier">)</LI><LI>Maximum (</FONT><FONT FACE="Courier" SIZE=2>heap_max()</FONT><FONT FACE="Courier">)</LI><LI>Extract Maximum (</FONT><FONT FACE="Courier" SIZE=2>heap_extract_max()</FONT><FONT FACE="Courier">)</LI></UL></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P>I will discuss the implementation and running time of each operation in turn.</P><B><P>2.1.1 Insert</P></B></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P>The heap insert operation is implemented exactly as described by <U>Cormen, Leierson and Rivest</U>[1].</P><P>The only modification is that the </FONT><FONT FACE="Courier" SIZE=2>compare_priority()</FONT><FONT FACE="Courier"> function is used to compare the priorities of a node and its parent in place of a direct comparison.  This function is a wrapper around the provided library routine </FONT><FONT FACE="Courier" SIZE=2>compare()</FONT><FONT FACE="Courier">, and is required because we need to know only if a given key is greater than another key.  If two processes have equal priority, then their relative ordering is based on which process has the greater job id.</P><P>The running time of this operation is worst case O(lg(<I>n</I>)).  This running time occurs when a newly added node has a greater priority than every other node in the heap.  When such a node is added, it must be moved from its position as the last element in the heap to the first, by tracing a path from its current position to the root of the heap.  Since this path contains at most lg(<I>n</I>) nodes the worst case running time is O(lg(<I>n</I>)).</P><B><P>2.1.2 Delete</P></B></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P>The heap delete operation in my implementation is loosely based on the function HEAP_EXTRACT_MAX given by <U>Cormen, Leierson and Rivest</U>[1].  The process is as follows:</P><DIR><DIR></FONT><FONT FACE="Courier" SIZE=2><P>HEAP_DELETE(A,i) {</P><P>if (i &gt; heap_size(A) || i &lt; 1)</P><DIR><DIR><P>error &quot;No such element&quot;;</P></DIR></DIR></FONT><FONT FACE="Times" SIZE=2><P>&#9;</FONT><FONT FACE="Courier" SIZE=2>A[i] = A[heap_size(A)];</P><P>&#9;heap_size(A)--;</P><P>&#9;heapify(A,i);</P><P>}</P></FONT><FONT FACE="Times"></DIR></DIR></FONT><FONT FACE="Courier"><P>In order to remove the requested item, we move the last item in the heap to the position of the item to be deleted.  Once this operation completes, the requested item has been deleted from the heap allowing us to shorten the heap size by 1. At this point the heap property may be violated so we call </FONT><FONT FACE="Courier" SIZE=2>heapify()</FONT><FONT FACE="Courier"> to restore this property.  The </FONT><FONT FACE="Courier" SIZE=2>heapify()</FONT><FONT FACE="Courier"> function is implemented as given by <U>Cormen, Leierson and Rivest</U>[1], except for the previously noted change involving </FONT><FONT FACE="Courier" SIZE=2>compare_priority().</P></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P>The running time of this operation is bounded by the call to </FONT><FONT FACE="Courier" SIZE=2>heapify()</FONT><FONT FACE="Courier">.  The process of checking the element, moving the last element to the index of the item to be deleted and shortening the heap only constant time, <FONT FACE="Symbol">&#81;</FONT>(1).  The call to </FONT><FONT FACE="Courier" SIZE=2>heapify(),</FONT><FONT FACE="Courier"> will require O(lg(<I>n</I>)) in the worst case, thus the running time of this routine is O(<FONT FACE="Symbol">&#81;</FONT>(1) + O(lg(<I>n</I>)) = O(lg(<I>n</I>)).  This satisfies the worst case running time requirement imposed by <U>Cormen, Leierson and Rivest</U>[1].</P><B><P>2.1.3 Increase Key</P></B></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P>My implementation of the increase key operation exhibits similarities to the delete operation.  A short description of a key change operation is given by <U>Segdewick</U>[2], but it requires uses extra comparisons between keys to implement. My process is as follows:</P><DIR><DIR></FONT><FONT FACE="Courier" SIZE=2><P>HEAP_INCREASE_KEY(A,i,key) {</P><P>if (i &gt; heap_size(A) || i &lt; 1)</P><DIR><DIR><P>error &quot;No such element&quot;;</P></DIR></DIR></FONT><FONT FACE="Times" SIZE=2><P>&#9;</FONT><FONT FACE="Courier" SIZE=2>A[i] = key;</P><P>&#9;heapify(A,i);</P><P>}</P></FONT><FONT FACE="Times"></DIR></DIR></FONT><FONT FACE="Courier"><P>Here we set the key at the specified node to be given key, and then we call </FONT><FONT FACE="Courier" SIZE=2>heapify()</FONT><FONT FACE="Courier"> to restore any violations of the heap property.</P><P>This routine assumes that the specified <I>key</I> is greater than the value currently stored at the index <I>i</I>.  The scheduler specification stated that this assumption would always hold true.  </P><P>The worst case running time of this routine is bounded by the call to </FONT><FONT FACE="Courier" SIZE=2>heapify()</FONT><FONT FACE="Courier">.  The error checking and the key assignment both require only constant time, <FONT FACE="Symbol">&#81;</FONT>(1).  The call to </FONT><FONT FACE="Courier" SIZE=2>heapify(),</FONT><FONT FACE="Courier"> will require O(lg(<I>n</I>)) in the worst case, thus the running time of this routine is O(<FONT FACE="Symbol">&#81;</FONT>(1) + O(lg(<I>n</I>)) = O(lg(<I>n</I>)).  This satisfies the worst case running time requirement imposed by <U>Cormen, Leierson and Rivest</U>[1].</P><B><P>2.1.4 Maximum</P></B></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P>The maximum operation is implemented to return a pointer to the first node in the heap.  If the heap is empty it returns a null pointer.  Since this operation requires only one access it runs in constant time, <FONT FACE="Symbol">&#81;</FONT>(1).</P><B><P>2.1.5 Extract Maximum</P></B></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P>The extract maximum operation is implemented to return the topmost node and then restore the heap property using </FONT><FONT FACE="Courier" SIZE=2>heapify()</FONT><FONT FACE="Courier">.  My implementation is the same as the one given by <U>Cormen, Leierson and Rivest</U>[1].</P><P>In this routine the call to </FONT><FONT FACE="Courier" SIZE=2>heapify() </FONT><FONT FACE="Courier">dominates, requiring O(lg(<I>n</I>)) in the worst case. The running time of this is bounded by this and is thus O(lg(<I>n</I>)).</P><B><P>2.2 Best and Worst Case Inputs</P></B></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P>The heap is an extremely efficient data structure.  It is reasonably fast, since none of its operations require more than O(lg(<I>n</I>)) worst case time.</P><P>In my implementation all of the operations display this behavior, but there are hidden costs involved with the following commands that could cause slow performance:</P><UL><LI>KILL</LI><LI>NICE</LI><LI>AGE PROCESSES</LI></UL></FONT><FONT FACE="Times"></FONT><B><FONT FACE="Courier"><P>2.2.1 KILL and NICE</P></B></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P>Both the KILL and NICE operations are really bounded by O(<I>n</I> + lg(<I>n</I>)) and not O(lg(<I>n</I>)).  Although it is possible to delete or increase the key of any node from the heap and restore the heap property in O(lg(<I>n</I>)), these commands request that a node with a particular job id be acted on.  In order to act on the specified node, it must be found in the heap.</P><P>This involves searching the array in which the nodes are stored for a node with the given job id.  The worst case occurs when the search fails or when the requested job id belongs to the last item in the array.  These conditions can only occur after all <I>n</I> nodes in the array have been considered, thus the find operation has a O(<I>n</I>) worst case running time.</P><P>Due to this fact both the KILL and NICE commands can be slow. If a search fails these commands have <FONT FACE="Symbol">&#81;</FONT>(<I>n</I>) running time, but if a search succeeds, then the commands can have a worst case running time of O(<I>n</I> + lg(<I>n</I>)).  Inputs that always perform KILL or NICE operations on the last item in a large heap will increase the running time of scheduler.</P><P>Two schemes were considered to increase the performance of the find operation:</P><UL><LI>A Hash Table of Job ID&#146;s</LI><LI>A Binary Tree of Job ID&#146;s</LI></UL></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P>If a hash table were maintained then the node corresponding to a particular job id could be located in <FONT FACE="Symbol">&#81;</FONT>(1<I> + <FONT FACE="Symbol">&#97;</FONT></I>) time, assuming simple uniform hashing and collision resolution via chaining.  In this hash table the key would be job id for a process and the stored value would be that job&#146;s index in the heap.  Though two jobs with the same id will not run concurrently, depending on the hash function used, two jobs with different job id&#146;s may hash the same.</P><P>The alternative to the hash table was to maintain a binary search tree of processes, where the key would be the job id and value of a particular key would be the index of the node corresponding to that job id in the heap.  The binary search tree is attractive because all the operations that we will perform (search, insert and delete) run in O(<I>h</I>) = O(lg(<I>n</I>)) time.  Thus we could find the index of a given job id much faster than the find method we now employ.</P><P>Both of these solutions, and indeed any solution that speeds up the find operation, slow down the entire system.  This is because any change to the heap requires a change to the auxiliary structure.  Thus a call to heapify(), which may touch lg(<I>n</I>) nodes, will require lg(<I>n</I>) nodes of the auxiliary structure to be updated.  This fact is lost in the O notation, since the total cost of heapify() would be O(lg(<I>n</I>) + lg(<I>n</I>)) = O(lg(<I>n</I>).  In reality, the constants will be significant.</P><P>To justify implementing either scheme, the majority of inputs (&gt; lg(n)) would have to be NICE or KILL requests for the last node in the heap.  Even in this case, the amount of extra code required for these schemes is almost as much as the code which is required to implement the scheduler itself, thus it hard to justify replacing the short and simple find routine with something much more complex, but not much more efficient.</P><B><P>2.2.2 Age Processes</P></B></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P>The aging of processes every few cycles presented an opportunity for improvement.  The current implementation ages each of the processes then calls the BUILD_HEAP routine described by <U>Cormen, Leierson and Rivest</U>[1], in order to restore the heap property.  This routine requires O(<I>n</I>) time to run in the worst case.  There is a modification to this routine, called BUILD_HEAP_PRIME&lt; suggested by <U>Cormen, Leierson and Rivest</U>[1].  This modification runs in <FONT FACE="Symbol">&#81;</FONT>(<I>n</I>(lg(<I>n</I>)) time.</P><P>It is obvious that for large values of <I>n</I>, the standard BUILD_HEAP will be slower than BUILD_HEAP_PRIME, thus an ideal implementation would be one that switched to BUILD_HEAP_PRIME from BUILD_HEAP at some critical value of <I>n</I>.  In order to determine this critical value, I conducted some tests to determine the constants that were hidden by the <FONT FACE="Symbol">&#81;</FONT> and O notations.  Since the results of these tests were inconclusive, I have chosen not to implement a switch between these two routines.</P><B><P>2.3 Comparision</P></B></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P>The primary reasons for choosing the heap as the data structure for my priority queue were:</P><UL><LI>Ease of implementation</LI><LI>Efficiency (all major operations require O(lg(<I>n</I>)) time.)</LI></UL><UL><LI>Elements located compactly in consecutive memory locations.</LI></UL></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P>First, of all the data structures considered, the heap was the simplest to implement and debug.  Since it is a heavily studied data structure, documentation was readily available.  This proved invaluable because the correctness of all the major operations could be easily verified with known examples.</P><P>In addition, each of the operations are based on simply introducing a violation of the heap property, and then using a routine to restore the heap property.  This meant that once the </FONT><FONT FACE="Courier" SIZE=2>heapify()</FONT><FONT FACE="Courier"> routine started functioning correctly, each of the other operations could be implemented in short order.</P><P>Second, the heap is a very efficient data structure in terms of the running time of its operations.  The major operations (HEAP_INSERT, HEAP_DELETE, HEAP_INCREASE_KEY), except BUILD_HEAP, run in O(lg(<I>n</I>)) time.  BUILD_HEAP runs in O(<I>n</I>) time, but even this is quite good.  Also, the heap provides instant access to the node with the highest priority.  Of the other data structures available, some (eg Leftist Heaps and Fibonacii Heaps) allowed for the required operations to be performed faster, but they required more memory and were much harder to debug and verify.</P><P>The memory requirement was the third consideration. The heap provided the most compact memory image.  All of the heap elements are stored adjacently in memory, and no additional space is required for moving records around.  By comparison, leftist heaps provide slightly faster access but at the cost of extra memory.  For large heaps, standard binomial heaps can handle the more nodes in a smaller memory.  Though Fibonacii Heaps give better performance when the heap is large, they require more memory than binomial heaps, thus fewer nodes can be represented.  In addition, for small heaps, the binomial heap provides on par performance, while requiring a very small memory image.</P><B><P>3.0 Acknowledgements</P></B></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P>I would like to thank Bharathi Raghavan, Mark Sapsford and Lothar Groth for their invaluable assistance. I would not have been able to complete this assignment without their help in debugging and their wisdom regarding implementation details.</P><B><P>3.1 References</P></B></FONT><FONT FACE="Times"></FONT><FONT FACE="Courier"><P><OL><LI>Cormen, Thomas H., Lierson, Charles E., Rivest, Ronald L. <U>Introduction to Algorithms</U>. MIT Cambridge Press, 1990.  Pages 140-152.</P><P><LI>Sedgewick, Robert. <U>Algorithms in C</U>. Addison-Wesley Publishing Company, 1990. Pages 145-153.</P><P><LI> Williams, J. Algorithm 232 (heap sort). <U>Communications of the ACM</U>. 7:347-348, 1964.</P><P><LI> Knuth, Donald. <U>The Art of Computer Programming, Volume 3: Sorting and Searching</U>. Addison-Wesley Publishing Company, 1973. Pages 149-153.</OL></P></FONT>
<CENTER><FONT SIZE=-1>Revision Information: <i>$Id: index.html 822 2006-12-05 03:15:35Z ranga $</i></FONT></CENTER>
</BODY></HTML>





